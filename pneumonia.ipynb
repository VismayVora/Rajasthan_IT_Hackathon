{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "UIX-yM5gWBeK"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization, Conv2D,MaxPool2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "0C1Q3d7BWBeK"
   },
   "outputs": [],
   "source": [
    "#Dataset: https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia\n",
    "train_path = \"chest_xray/train/\"\n",
    "test_path = \"chest_xray/test/\"\n",
    "validation_path = \"chest_xray/val/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KlNcYbG5WBeK"
   },
   "outputs": [],
   "source": [
    "def process_data(img_dims, batch_size):\n",
    "    # create three Data generation objects for train test and validation\n",
    "    train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "    val_datagen = ImageDataGenerator(rescale=1./255) \n",
    "\n",
    "    # Code for train,val,test batches which will be fed to the network in the specified batch sizes and image dimensions\n",
    "    train_gen = train_datagen.flow_from_directory(\n",
    "    directory=train_path, \n",
    "    target_size=(img_dims, img_dims), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary', \n",
    "    shuffle=True)\n",
    "\n",
    "    test_gen = test_datagen.flow_from_directory(\n",
    "    directory=test_path, \n",
    "    target_size=(img_dims, img_dims), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary', \n",
    "    shuffle=True)\n",
    "\n",
    "    val_gen = val_datagen.flow_from_directory(\n",
    "    directory=validation_path, \n",
    "    target_size=(img_dims, img_dims), \n",
    "    batch_size=batch_size, \n",
    "    class_mode='binary', \n",
    "    shuffle=True)\n",
    "    \n",
    "    return train_gen,val_gen,test_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k5ALEctPWBeL",
    "outputId": "84dd1027-3d07-45aa-9b55-44fdf793964c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "img_dims = 150\n",
    "epochs = 10\n",
    "batch_size = 32\n",
    "\n",
    "# Getting the data\n",
    "train_gen, val_gen , test_gen = process_data(img_dims, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "L5Js_NMWWBeL"
   },
   "outputs": [],
   "source": [
    "def plotImages(images_arr):\n",
    "    fig, axes = plt.subplots(1, 10, figsize=(30,30))\n",
    "    axes = axes.flatten()\n",
    "    for img, ax in zip( images_arr, axes):\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 186
    },
    "id": "vjfawizoWBeM",
    "outputId": "5bf670a4-8bb4-4873-86cb-97c07c3f5f0c"
   },
   "outputs": [],
   "source": [
    "imgs, labels = next(train_gen)\n",
    "plotImages(imgs)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G8HEf8taWBeM"
   },
   "outputs": [],
   "source": [
    "def model():\n",
    "    model = Sequential([\n",
    "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', padding = 'same', input_shape=(150,150,3)),\n",
    "    BatchNormalization(),  #--------------batch normalization--------#\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation=\"relu\", padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(2, 2)),\n",
    "    Conv2D(128, (3, 3), activation=\"relu\", padding='same'),  \n",
    "    BatchNormalization(),\n",
    "    MaxPool2D(pool_size=(3, 3)),\n",
    "    Flatten(),\n",
    "    Dense(120, activation='relu'),\n",
    "    Dense(60, activation='relu'),\n",
    "    Dropout(rate=0.2),    #---------dropout-----------------------#\n",
    "    Dense(1, activation='sigmoid') ,\n",
    "\n",
    "    ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TJ0z-EIhWBeM",
    "outputId": "9ce7b688-0d41-4ba0-ad6d-cbdf6a686b27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 150, 150, 32)      896       \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 150, 150, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 75, 75, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 75, 75, 64)        18496     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 75, 75, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 37, 37, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 37, 37, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 37, 37, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 12, 12, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 18432)             0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 120)               2211960   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 60)                7260      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 60)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 61        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,313,425\n",
      "Trainable params: 2,312,977\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "flOGjjdzWBeN"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=\"accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PjV1StWIWBeN",
    "outputId": "a1290479-544e-4264-9b97-9536c7399658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-20 01:42:20.663394: W tensorflow/tsl/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 - 58s - loss: 0.2173 - accuracy: 0.9331 - val_loss: 11.4991 - val_accuracy: 0.6250 - 58s/epoch - 354ms/step\n",
      "Epoch 2/10\n",
      "163/163 - 61s - loss: 0.0948 - accuracy: 0.9655 - val_loss: 8.5816 - val_accuracy: 0.6283 - 61s/epoch - 374ms/step\n",
      "Epoch 3/10\n",
      "163/163 - 63s - loss: 0.0732 - accuracy: 0.9739 - val_loss: 2.4836 - val_accuracy: 0.6711 - 63s/epoch - 387ms/step\n",
      "Epoch 4/10\n",
      "163/163 - 64s - loss: 0.0647 - accuracy: 0.9774 - val_loss: 7.9780 - val_accuracy: 0.6283 - 64s/epoch - 392ms/step\n",
      "Epoch 5/10\n",
      "163/163 - 63s - loss: 0.0499 - accuracy: 0.9818 - val_loss: 13.4198 - val_accuracy: 0.6283 - 63s/epoch - 387ms/step\n",
      "Epoch 6/10\n",
      "163/163 - 63s - loss: 0.0478 - accuracy: 0.9816 - val_loss: 3.4738 - val_accuracy: 0.7039 - 63s/epoch - 388ms/step\n",
      "Epoch 7/10\n",
      "163/163 - 63s - loss: 0.0357 - accuracy: 0.9858 - val_loss: 7.0061 - val_accuracy: 0.6283 - 63s/epoch - 389ms/step\n",
      "Epoch 8/10\n",
      "163/163 - 63s - loss: 0.0458 - accuracy: 0.9826 - val_loss: 4.4880 - val_accuracy: 0.6480 - 63s/epoch - 387ms/step\n",
      "Epoch 9/10\n",
      "163/163 - 64s - loss: 0.0321 - accuracy: 0.9889 - val_loss: 0.6584 - val_accuracy: 0.8553 - 64s/epoch - 392ms/step\n",
      "Epoch 10/10\n",
      "163/163 - 64s - loss: 0.0256 - accuracy: 0.9898 - val_loss: 4.8233 - val_accuracy: 0.6382 - 64s/epoch - 391ms/step\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(patience=15, monitor='val_accuracy', restore_best_weights=True)\n",
    "\n",
    "hist= model.fit(train_gen, steps_per_epoch=train_gen.samples // batch_size, \n",
    "           epochs=epochs, validation_data=test_gen, \n",
    "           validation_steps=test_gen.samples // batch_size,\n",
    "           verbose=2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "wtDgCQtAWBeN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r+)>) saving:\n",
      "...layers\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dropout\n",
      ".........vars\n",
      "......flatten\n",
      ".........vars\n",
      "......max_pooling2d\n",
      ".........vars\n",
      "......max_pooling2d_1\n",
      ".........vars\n",
      "......max_pooling2d_2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........29\n",
      ".........3\n",
      ".........30\n",
      ".........31\n",
      ".........32\n",
      ".........33\n",
      ".........34\n",
      ".........35\n",
      ".........36\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n",
      "Keras model archive saving:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-03-20 01:53:57         6080\n",
      "metadata.json                                  2023-03-20 01:53:57           64\n",
      "variables.h5                                   2023-03-20 01:53:57     27818400\n",
      "Keras model archive loading:\n",
      "File Name                                             Modified             Size\n",
      "config.json                                    2023-03-20 01:53:56         6080\n",
      "metadata.json                                  2023-03-20 01:53:56           64\n",
      "variables.h5                                   2023-03-20 01:53:56     27818400\n",
      "Keras weights file (<HDF5 file \"variables.h5\" (mode r)>) loading:\n",
      "...layers\n",
      "......batch_normalization\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......batch_normalization_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "............2\n",
      "............3\n",
      "......conv2d\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......conv2d_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_1\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dense_2\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......dropout\n",
      ".........vars\n",
      "......flatten\n",
      ".........vars\n",
      "......max_pooling2d\n",
      ".........vars\n",
      "......max_pooling2d_1\n",
      ".........vars\n",
      "......max_pooling2d_2\n",
      ".........vars\n",
      "...metrics\n",
      "......mean\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "......mean_metric_wrapper\n",
      ".........vars\n",
      "............0\n",
      "............1\n",
      "...optimizer\n",
      "......vars\n",
      ".........0\n",
      ".........1\n",
      ".........10\n",
      ".........11\n",
      ".........12\n",
      ".........13\n",
      ".........14\n",
      ".........15\n",
      ".........16\n",
      ".........17\n",
      ".........18\n",
      ".........19\n",
      ".........2\n",
      ".........20\n",
      ".........21\n",
      ".........22\n",
      ".........23\n",
      ".........24\n",
      ".........25\n",
      ".........26\n",
      ".........27\n",
      ".........28\n",
      ".........29\n",
      ".........3\n",
      ".........30\n",
      ".........31\n",
      ".........32\n",
      ".........33\n",
      ".........34\n",
      ".........35\n",
      ".........36\n",
      ".........4\n",
      ".........5\n",
      ".........6\n",
      ".........7\n",
      ".........8\n",
      ".........9\n",
      "...vars\n"
     ]
    }
   ],
   "source": [
    "pickle.dump(model,open('pneumonia_model.pkl','wb'))\n",
    "model = pickle.load(open('pneumonia_model.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 20ms/step\n",
      "[1.]\n",
      "pneumonia\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "img_path = 'chest_xray/test/PNEUMONIA/person1_virus_6.jpeg'\n",
    "img = image.load_img(img_path, target_size=(150, 150))\n",
    "img = np.array(img)\n",
    "img = np.expand_dims(img, axis=0)\n",
    "img = img/255.0\n",
    "\n",
    "# make a prediction\n",
    "prediction = model.predict(img)[0]\n",
    "print(prediction)\n",
    "if prediction > 0.5:\n",
    "    prediction = 'pneumonia'\n",
    "else:\n",
    "    prediction = 'normal'\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "uia-.venv",
   "language": "python",
   "name": "uia-.venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
